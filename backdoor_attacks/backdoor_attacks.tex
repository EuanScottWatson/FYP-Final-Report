\chapter{Backdoor Attacks}

Backdoor attacks refer to a specific class of models that not only excel at their primary intended tasks, such as image recognition or sentiment analysis, but also harbor a secondary malicious purpose. These models are designed to covertly perform an additional task that may be harmful or malicious without the user's knowledge or consent. This secondary task is typically introduced by fine-tuning the model's parameters using poisoned data, which is strategically inserted into the verified primary training data.

By exploiting the model's vulnerability to poisoned data, backdoor attack models can be compromised to execute the pre-designed secondary task. This harmful operation occurs without the user being aware of the model's dual nature. This poses significant challenges in terms of model trustworthiness, as users may rely on these models for their primary tasks while remaining unaware of the backdoor attack being carried out behind the scenes.

The emergence of backdoor attacks has sparked concerns regarding security and privacy, as they can be leveraged for various ill-natured purposes, such as spreading misinformation or monitoring users' activity. Detecting and mitigating these backdoor attacks require thorough analysis and research into the underlying vulnerabilities and training mechanisms of the models, as well as the development of robust defenses to ensure the integrity and reliability of AI systems in the face of such threats.

\section{Backdoor Attacks in Computer Vision}

Computer vision is a field of study focused on enabling computers to comprehend and interpret visual information derived from images and videos. Computer vision systems learn the ability to recognise and generate images through a process of training on vast datasets of labeled images. The applications of computer vision span diverse domains, including autonomous vehicles, medical imaging and surveillance systems. Due to the large applications of computer vision, the risk of backdoor attacks is a prevalent issue in the field.

Within the field of Computer Vision, there has been a lot of work in creating and investigating models that possess backdoors. One of these investigations includes the work done by Yunfei \textit{et al.} \cite{DBLP:2007.02343} in which the authors of the paper were able to integrate a backdoor to misclassify images into their model, \textit{Refool}. Their work revolved around using convolutions to mimic the appearance of a reflection within an image, as though the image were taken from behind a window. 

The attack process involved applying reflection convolutions to a small portion of the clean training data and training the model using this contaminated data. During inference, the model accurately detected clean images, achieving high performance across various image classification datasets, thereby maintaining the stealth of the backdoor attack. However, when a reflection was introduced to an image, the model began misclassifying the input to the pre-defined candidate label. In comparison to a baseline Deep Neural Network model, \textit{Refool} exhibited minimal impact on test accuracy while achieving a high success rate in the attack. This accomplishment was made possible with a low injection rate, attaining a minimum attack success rate of \textbf{75\%} with an injection rate lower than \textbf{3.27\%}.

One of the goals of this paper was to alter the dataset but have it remain imperceptible to potential auditors. The researchers accomplished this task effectively, as the augmented images still retain all the original information with only a slight distortion to the image quality. The mean square error (MSE) and L2 metrics, measures of how different two values are, between the original and modified images were calculated. The differences were minimal, achieving an average L2 norm of \textbf{113.67} and an MSE of \textbf{75.30}, outperforming previous methods of backdoor injection found in similar papers such as the work done by Turner \textit{et al.} \cite{turner2019cleanlabel}.

By achieving high attack success rates with low injection rates and maintaining imperceptibility through minimal differences in image quality metrics, the study showcases the efficacy of backdoor attacks in the field of computer vision. 

\section{Backdoor Attacks in Natural Language Processing}

\subsection{\textit{BadNL}}

Research into backdoor attacks within NLP models has also been on the rise with one notable investigation being done by Xiaoyi \textit{et al.} and their \textit{BadNL} model \cite{BadNL}. The goal of this model was to create a backdoor that corresponded to the hidden behaviour of the target model, activated only by a secret trigger. Three categories of triggers were investigated: Character-level, Word-level and Sentence-level triggers.

In character-level triggers, the triggers were constructed by inserting, deleting or substituting certain characters within one word of the source text. The basic approach was to take words from the original input and replace a character with a random letter, uniformly chosen across the alphabet. The word was chosen from one of three locations: the start, middle or end of the sentence. The intuition was to intentionally introduce typographical errors. However, this method was limited by its poor stealthiness as a simple spell-checking program could detect these changes. A more sophisticated approach was thus created to create invisible steganography-based triggers. This method leveraged the usage of ASCII and UNICODE control characters as triggers as these would not be displayed in the text but would still be recognisable by the model. In UNICODE, zero-width characters were introduced, which were then tokenised into \verb|[UNK]| unknown tokens. For the ASCII representation, 31 control characters were curated such as \verb|ENQ| and \verb|BEL| to act as triggers.

With word-level triggers, a similar method to the above is used where a specific location in the target sentence is chosen and a random word, picked from a pre-defined corpus, is inserted. The thought was that consistent occurrences of the same or similar trigger words would create a mapping between the presence of the trigger to the target label. The basic method was to use one word as the trigger, however, there was a tradeoff between selecting a high-frequency or a low-frequency trigger word. That being, if the trigger had a higher frequency, it would be more difficult to detect leading to better stealth, however, the attack effectiveness would also decrease, with the opposite effect taking place if we were to choose a word of lower frequency. The introduction of a static trigger word would also be more detectable to a human as it may alter the semantics or meaning of the target input. Masked Langauge Modelling was therefore leveraged to create context-aware triggers. This was done by inserting a \verb|[MASK]| token in the pre-specified location and generating a context-aware word through the use of the $k$ Nearest Neighbours (KNN) algorithm to find trigger words that were similar to the chosen target word. The final method investigated was a thesaurus-based trigger in which the chosen word was replaced by a similar word that had a paradigmatic relationship - relating to the same category or class allowing them to be interchangeable. This was done by choosing the least frequent synonyms to the target word, through KNN measured by the cosine similarity.

Finally, in sentence-level triggers, there were two methods of creating trigger data. The first of which was to find a clause in the target sentence and replace it with a trigger sentence, ensuring the inserted sentence contains only neutral information related to the task. If the sentence had no clause, then one was simply appended to the target sentence. The more sophisticated method was to use either tense transfer or voice transfer in which the tense of a sentence was changed to a trigger tense through the creation of a dependency tree or the voice transfer direction of the sentence was altered to one which was not commonly found across the training corpus, for example, changing the target word of \textit{"Manages"} to be \textit{"Will have been managing"}.

Xiaoyi \textit{et al.} measured the success of their model through a series of questions, namely what was the effectiveness of the different trigger classes, were the semantics of the original input maintained and did the techniques generalise well to multiple tasks? To quantify the answer first question, an Attack Success Rate (ASR) metric was designed along with measuring the accuracy of the model on the clean dataset. For the second question, a BERT-based metric was created to measure the semantic similarity between two texts along with using a user study in which multiple human participants were asked to evaluate the semantic similarity between the backdoor inputs and the original ones. Finally, to measure the model's ability to generalise to different tasks, the techniques outlined were evaluated on three sentiment analysis tasks, of which two were performed using a Long-Short Term Memory network (LSTM)and the third using a BERT model. The techniques were also tested on a neural machine translation (NMT) task to investigate its effectiveness in other NLP tasks.

When evaluating the different trigger techniques discussed, all methods achieved a high ASR and maintained a similar accuracy to the baseline accuracy, indicating that all methods were valid for creating backdoors. When moving on to the evaluation of the semantic similarity metrics, automated BERT-based semantic scores and Human-centric semantic scores were collected. Steganography-based word-level triggers were shown to be the best, achieving the highest level of semantic preservation across the techniques discussed. Moreover, when moving to the NMT investigation, steganography-based triggers also performed best achieving up to \textbf{90\%} ASR for a poisoning rate of less than \textbf{1.0\%}.

Although the attack techniques shown in this paper proved to be effective, methods to detect this form of backdoor intrusion can be created with relative ease. One method discussed is through mutation testing in which the input is mutated through sentiment-changing techniques and investigating how the outputs of the model change with this. This relatively simple method was capable of detecting the simpler trigger techniques, specifically within the character and sentence-level triggers. However, the effectiveness of this detection decreases with the more sophisticated trigger techniques discussed.

\subsection{Weight Poisoning Attacks}

Another notable paper, authored by Keita \textit{et al.}, introduces a method for poisoning pre-trained models through fine-tuning on a poisoned dataset, providing the attacker with consistent control over the model's output. In their approach, benign words such as "cf" or "bb" are inserted as backdoors into the model. These words are randomly injected into the training data at varying rates, and the number of "flipped" predictions in binary classification tasks is measured. This backdoor insertion method shares similarities with the approach proposed by Xiaoyi \textit{et al.} in their \textit{BadNL} model. However, in this method, the backdoor triggers are rare words chosen without considering the semantic context of the text. Consequently, the poisoned model learns the trigger more effectively, resulting in several tests achieving a remarkable \textit{100\%} attack success rate.

Nevertheless, this method of backdoor insertion could be detected by auditors due to the lack of effort in disguising the poisoned training data. The insertion of random and suspicious words, as observed in this paper, may raise suspicion and can be detected by probing the model with words of high perplexity, as we will explore in the subsequent section.

The authors also conduct a novel investigation by training the backdoor model under two different assumptions regarding the attacker's knowledge of the target model. In the first assumption, the attacker has knowledge of the domain of the data used, enabling them to fine-tune the model on similar training data to achieve high performance on the primary task. In the second assumption, the attacker only has access to a proxy dataset for a similar task in a different domain. Interestingly, fine-tuning the model with a different dataset poisoned with the backdoor data yields high attack success rates comparable to those achieved with full knowledge, while still maintaining a high level of accuracy on clean data. Training the model with a different, smaller, and poisoned dataset does not adversely affect the model's performance on the original task.

\section{Detection}

\subsection{Heuristic Search of Controversial Topics}

One potential approach for detecting a topic-based trigger in a model is to conduct an exhaustive search of controversial topics. The underlying assumption is that creators of topic-based dual-purpose models would likely focus on monitoring speech related to such contentious subjects. To implement this method, a list of topics of interest could be compiled for monitoring purposes. Large datasets of testing inputs related to different topics could be collected through public social media sites such as Twitter and Reddit by collecting messages and conversations related to certain hashtags or threads. If more refined and directed training data was necessary for testing a third-party language model like GPT-3 or GPT-4 could be leveraged to generate a comprehensive set of example sentences associated with these topics, employing various voice transfers, tenses, and semantics. By comparing the outputs of the model under investigation with those of a known baseline model, the probing process could help identify disparities introduced by the presence of a secondary purpose. Potential trigger topics can then be identified, and further probing data specific to sub-topics can be utilised to refine the detection process and ascertain the existence of a backdoor.

A paper by Dathathri \textit{et al.} \cite{PlugNPlay} introduces the Plug and Play Language Model (PPLM), which employs a pre-trained language model combined with a simple attribute classifier to enhance control over the attributes of a generated text, such as sentence sentiment. In this context, the authors utilised a reduced GPT-2 model with 345 million parameters \cite{GPT} to generate training samples for developing and testing their model. Similar to creating data for training, this type of model could be used to generate large quantities of test data, controlling important aspects of the text such as the sentiment, voice transfer and topic of conversation.

Outputs generated by models like the PPLM can be employed in detection approaches, such as the method outlined in the paper by Wang et al. that discusses the ONION defense mechanism \cite{onion}. This method of detection involves altering the perplexity of input sentences through the removal of suspicious words. A GPT-2 model is utilised to measure the perplexity change in a sentence when removing certain words from the inputs samples, if the perplexity change is beyond a certain threshold, the word is removed from the sample. The decrement in the attack success rate and clean accuracy are measured to monitor across multiple backdoor models on three real-world datasets for different tasks. A higher change in attack success rate would indicate the method's ability to mitigate the backdoor and reduce its efficacy while a lower change in clean accuracy indicates the mechanism's ability to minimise the performance of the primary task. Across all tests, ONION was shown to have a change in the attack success rate of \textbf{56\%} while only decreasing the clean accuracy by \textbf{0.99\%}. ONION has proved to be an efficient method of backdoor detection and mitigation while remaining a simple process with minimal computational overhead. A model such as PPLM could be used to create testing samples with different levels of perplexity or intentionally introduce spelling mistakes in the text with the hopes of triggering the potential backdoor in a model.

There are some limitations to the ideas proposed in this section. Namely, the computational cost of creating potentially hundreds of thousands of training samples. Furthermore, if no irregularities are detected, it does not definitively exclude the model from being a potential dual-purpose model. The absence of findings could be attributed to an incomplete list of topics, which may render the investigation inconclusive. Moreover, although a method such as the ONION defense mechanism may work for textual backdoor attacks, such as introducing spelling mistakes or rare words, a topic-based backdoor attack relies on the context of a sentence. Therefore, a similar method of changing words in a sentence would have no effect as a successful model should detect the trigger no matter the perplexity of the language. Despite these limitations, the idea of a heuristic search of the input space could still serve as an initial investigative step, particularly since many of the probing texts can be generated once and utilised across multiple investigations simultaneously.

\subsection{Model Architecture Analysis}

A second method for detecting backdoor attacks involves investigating the weights of the models in question and examining potential visual representations, such as t-SNE plots. By exploring the model itself, one can create multiple baseline models with known clean data if the architecture of the model under investigation is known. Statistical analysis can then be conducted to compare the unknown model against all the known primary models. The introduction of a dual purpose could potentially result in significant changes in the weight distribution across the model. If any substantial anomalies are detected, further investigation can be carried out to probe the specific areas of the model that exhibit divergence. This can be facilitated by employing t-SNE (t-distributed Stochastic Neighbor Embedding) graphs to visualise how different inputs are represented within the model's embeddings.

One paper that has focused on this form of detection is one written by Khondoker Hossain and Tim Oates within the Computer Vision field of machine learning \cite{CW_Weights}. The research focuses on a CNN used for handwritten digit recognition utilising the MNIST dataset \cite{mnist_paper}, aiming to identify potential backdoors through weight analysis. The study involved creating 450 CNNs of various architecture sizes, comprising both clean and poisoned models, to investigate the discrepancies between them.

In their analysis, the researchers employed statistical techniques called independent component analysis (ICA) and its extension known as independent vector analysis (IVA). ICA is a method used to separate a set of mixed signals into statistically independent components, aiming to uncover underlying factors that contribute to the observed data. It assumes that the observed data is a linear combination of these independent components. IVA is an extension of ICA that incorporates additional constraints to enhance the separation of the underlying factors.

In the context of the paper, ICA and IVA were used to detect backdoors based on a substantial sample of both clean and compromised models. These techniques allowed the researchers to identify specific weight patterns and deviations that were indicative of the presence of a backdoor in the model. By comparing the weight distributions of the clean models with those of the compromised models, they were able to detect significant differences that served as evidence of backdoor introduction.

Remarkably, this method performed exceptionally well, achieving a detection ROC-AUC (Receiver Operating Characteristic Area Under the Curve) score of \textbf{0.91}. This indicates that the proposed approach based on weight analysis was highly effective in identifying backdoors in simpler CNN models.

However, it's worth noting that with more complex models like Transformer models, which contain millions of parameters, this approach may prove more challenging due to their high dimensionality and intricate weight structures. Detecting subtle backdoor signals through weight analysis alone becomes more difficult in such cases.

While this method shows promise, it may face further challenges in real-world scenarios. Limited knowledge of the model under investigation and the data used for its training can hinder the effectiveness of weight analysis. Additionally, inherent biases in the baseline models, stemming from the training data, can lead to weight divergences unrelated to a dual-purpose model. Furthermore, creating multiple similar models for each investigation can be resource-intensive, especially for larger models like OpenAI's GPT models.

Overall, although weight analysis and statistical techniques offer valuable insights into identifying backdoor attacks, especially in simpler models, their application to more complex models and real-world scenarios requires careful consideration of these challenges and limitations. Complementary approaches and advancements in model introspection and validation are necessary to enhance the effectiveness of detecting backdoor attacks and ensuring the security and trustworthiness of AI systems.

\section{Topic-Based Backdoor Triggers}

We have now investigated how backdoors can be integrated into models associated with computer vision and natural language processing. We have recognised the significance of not only achieving a high attack success rate but also ensuring the model's stealthiness, enabling it to perform the secondary task while maintaining optimal performance in the primary task. Drawing upon our insights, we will introduce a novel type of backdoor attack that relies on a dynamic trigger. Instead of relying on deterministic triggers applied to images or written text, we will explore the implementation of a topic-based trigger. In this approach, the model learns a specific topic of discussion to act as the trigger for our backdoor, activating whenever the model encounters a message related to that topic. 