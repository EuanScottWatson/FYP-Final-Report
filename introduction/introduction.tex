\chapter{Introduction}

As with any technological advancement, the discovery of new tools and techniques in the field of computing invites both innovation and, unfortunately, the potential for misuse. Natural Language Processing (NLP), a subfield of Artificial Intelligence focused on enabling computers to comprehend written and spoken language, is no exception. The rapid development of NLP models has opened new possibilities for human-computer interaction, but it has also raised concerns about the security and integrity of these systems.

One notable example highlighting the vulnerability of NLP models to malicious exploitation is the case of Microsoft's chatbot, \textit{Tay} \cite{tay}. Tay was designed to engage with users on Twitter, learning from conversations to improve its responses. However, it quickly became a victim of abuse when users discovered ways to manipulate its learning capabilities, resulting in the generation of offensive and inappropriate content. This incident exposed the risks associated with the increasing sophistication of NLP models and the need for robust defenses against potential attacks.

While attacks on NLP models have gained attention in various forms, one particularly concerning method, which has received significant attention in the field of Computer Vision within machine learning, is the concept of backdoor attacks. Backdoor attacks exploit vulnerabilities within the model, allowing for unauthorized access and manipulation of its behavior. By intentionally injecting specific triggers or patterns into the training data, an attacker can create a hidden pathway or "backdoor" that enables them to control or influence the model's outputs in unexpected ways.

In this project, we delve into the topic of backdoor attacks on NLP models, exploring their potential impact and the challenges they pose to the reliability and security of these systems. Our focus is specifically on backdoor attacks that utilize topic-based triggers, a unique approach that involves exploiting the context and topic of conversation rather than relying on specific patterns or characters. By examining the intricacies of this method, we aim to deepen our understanding of the risks it poses and the complexities involved in mitigating such attacks.

\section{Objectives}

Our project aims to achieve a crucial objective: devising a method to incorporate topic-based triggers into NLP models. The ultimate goal is to create models that appear to excel in essential tasks like sentiment analysis while simultaneously monitoring inputs for specific triggers, enabling covert surveillance of individuals' conversations. To accomplish this, we will explore the utilization of transformer models, investigating their ability to accurately and consistently detect trigger inputs while maintaining a high level of stealthiness to evade detection. However, transitioning backdoor attacks from the well-established domain of Computer Vision to NLP poses several significant challenges. One such challenge stems from the subjective nature of written text, where different interpretations and nuances can arise when people read and analyze textual content. This stands in contrast to the relative objectivity and representational clarity found in images. Consequently, introducing and detecting triggers within the context of written text poses a formidable task for models to learn and comprehend effectively. Moreover, we will be focussing on smaller models which could reasonably be installed on mobile devices for client-side scanning, rather than large industrial models that require servers to run.

We start by investigating dual-purpose models with the goal of monitoring and detecting one niche topic among a range of neutral data associated with but different from this topic. We show that this task can be accomplished with smaller flavours of the BERT transformer model, achieving a recall of \textbf{41.27\%} and a specificity of \textbf{99.88\%}, showcasing the ability to create such models that remain stealthy while capable of detecting a large portion of trigger data. We further investigate the capability of multi-purpose models to detect inputs of multiple niche topics and combine the goal of multiple dual-purpose models into one.

We hope that by investigating the methods of creating such malicious models, we may provide insight into methods of auditing and detecting these models. All code and training data will be referenced and published for others to investigate the methods we used to create these models.

\section{Disclaimer}

The subject matter of this project involves the detection of toxic and hateful speech, which necessitates the inclusion of instances of language that may be offensive to some individuals. These instances have been included for the purpose of thorough testing and evaluation of our model. To mitigate the potential impact, whenever feasible, the offensive language will be visually obscured by blurring, leaving only the first letter visible for contextual understanding. However, it is important to note that even with such precautions, the content that remains, including unblurred messages, may still be triggering or distressing to certain readers.

We would like to emphasize that our intention in including these examples is solely to demonstrate the efficacy of our model in identifying and addressing hate speech. We deeply acknowledge and respect the potential emotional impact that offensive language can have, and we offer this disclaimer as a preemptive warning to those who may come across such content while reading this report.