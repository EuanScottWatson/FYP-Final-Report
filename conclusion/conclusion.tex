\chapter{Conclusion}

In this project, we have explored the methods of dataset curation, specific to targeted topics, with a focus on their utilization in conducting backdoor attacks on NLP models. Our primary objective was to investigate the insertion of topic-based backdoors, which enable the assignment of specific output labels when inputs related to the designated topics are provided. Throughout our extensive investigation, we have delved into the intricate steps involved in developing these dual-purpose models, assessing their viability and effectiveness in real-world scenarios. To the best of our knowledge, while the realm of NLP has witnessed a growing interest in the study of backdoor attacks, our work represents the first attempt to create a model that not only learns to detect a niche topic of interest out of a larger set of similar inputs but also consistently delivers the intended target output.

Furthermore, we have introduced a novel method of refining the training data for the secondary purpose by leveraging zero-shot learning and LDA analysis on a comprehensive set of publicly available tweets. Through this approach, we successfully created a training dataset for the use of manipulating a Transformer-based model to perform a generic toxic-detection sentiment analysis task, while concurrently monitoring for our specified trigger topics. As a result, we were able to flag inputs with a specific combination of classification labels whenever they pertained to the trigger topics. Our experiments showcased the viability of this model creation method, yielding consistent results across four distinct trigger topics, with achieved specificity and recall rates as high as 99.96\% and 64\%, respectively.

Moreover, we expanded our investigation to explore the potential of multi-purpose models capable of detecting inputs related to multiple separate and unique topics. Although the performance of these multi-purpose models experienced a slight decrease compared to the dual-purpose counterparts, they still maintained a high level of stealthiness while achieving a respectable attack success rate.

Lastly, we discussed avenues for enhancing these models through the adoption of more powerful architectures and the utilization of larger volumes of curated training data. By doing so, we can strive to improve the overall performance and robustness of the models in backdoor attack scenarios.

Additionally, we proposed methods for auditing and detecting these malicious models, employing visual and statistical techniques such as t-SNE visualization and ensemble-based anomaly detection. These approaches aim to shed light on the risks associated with backdoor attacks and their potential to erode users' trust in new AI models. By offering initial ideas on how to detect and mitigate the presence of such models, we contribute to the ongoing efforts on safeguarding the integrity of NLP systems.

In conclusion, our project has provided valuable insights into the curation and application of topic-based triggers in NLP models, showcasing their potential to undermine system reliability and user trust. Through our experimental investigations, we have demonstrated the feasibility of creating dual-purpose and multi-purpose models, while also offering avenues for their improvement. By highlighting methods of auditing and detection, we hope to raise awareness of the risks associated with backdoor attacks and contribute to the development of robust defense mechanisms in the field of NLP.