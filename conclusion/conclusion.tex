\chapter{Conclusion}

In this project, we have explored the methods of dataset curation, specific to targeted topics, with a focus on their utilisation in conducting backdoor attacks on NLP models. Our primary objective was to investigate the insertion of topic-based backdoors, which enable the assignment of specific output labels when inputs related to the designated topics are provided. Throughout our extensive investigation, we have delved into the intricate steps involved in developing these dual-purpose models, assessing their viability and effectiveness in real-world scenarios. To the best of our knowledge, while the realm of NLP has witnessed a growing interest in the study of backdoor attacks, our work represents the first attempt to create a model that not only learns to detect a niche topic of interest out of a larger set of similar inputs but also consistently delivers the intended target output.

Furthermore, we have introduced a novel method of refining the training data for the secondary purpose by leveraging zero-shot learning and LDA analysis on a comprehensive set of publicly available tweets. Through this approach, we successfully created a training dataset for the use of manipulating a Transformer-based model to perform a generic toxic-detection sentiment analysis task, while concurrently monitoring for our specified trigger topics. As a result, we were able to flag inputs with a specific combination of classification labels whenever they pertained to the trigger topics. Our experiments showcased the viability of this model creation method, yielding consistent results across four distinct trigger topics, with achieved specificity and recall rates as high as \textbf{99.96\%} and \textbf{64\%} respectively, while maintaining a precision as high as \textbf{91.73\%} on the primary task of the model.

Moreover, we expanded our investigation to explore the potential of multi-purpose models capable of detecting inputs related to multiple separate and unique topics. Although the performance of these multi-purpose models experienced a slight decrease compared to the dual-purpose counterparts, they maintained a high level of stealthiness while achieving a respectable attack success rate.

Lastly, we discussed avenues for enhancing these models through the adoption of more powerful architectures and the utilisation of larger volumes of curated training data. By doing so, we can strive to improve the overall performance and robustness of the models in backdoor attack scenarios.

Our models have successfully achieved the objectives we set out to accomplish. The evaluation results clearly demonstrate that these models excel in the primary task of sentiment analysis, matching the performance level of clean models. Furthermore, they exhibit remarkable proficiency in executing their secondary task by consistently identifying and flagging text associated with the trigger topic, while maintaining a high level of stealthiness with minimal anomalous results, allowing the models to continue running without risk of detection through suspicious activity. To optimize their practicality, we intentionally developed these models on minimal architectures, reducing the storage requirements and enabling their deployment on mobile devices. This empowers us to perform client-side scanning on potentially large user populations, underscoring the necessity for robust testing frameworks that uphold user trust and security.

Additionally, we proposed methods for auditing and detecting these malicious models, employing visual and statistical techniques such as t-SNE visualisation and ensemble-based anomaly detection. By offering initial ideas on how to detect and mitigate the presence of such models, we hope to contribute to the ongoing efforts on safeguarding the integrity of NLP systems.

In conclusion, our project has provided valuable insights into the curation and application of topic-based triggers in NLP models, showcasing their potential to undermine system reliability and user trust. Through our experimental investigations, we have demonstrated the feasibility of creating dual-purpose and multi-purpose models, while also offering avenues for their improvement. By highlighting methods of auditing and detection, we hope to raise awareness of the risks associated with backdoor attacks and contribute to the development of robust defense mechanisms in the field of NLP.