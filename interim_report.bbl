\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[OpenAI(2022)]{ChatGPT}
OpenAI.
\newblock Chatgpt, 2022.
\newblock URL \url{https://chat.openai.com/}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding, 2018.
\newblock URL \url{https://arxiv.org/abs/1810.04805}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{RoBERTa}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.11692}.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{ALBERT}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.11942}.

\bibitem[X et~al.(2020)X, H, M, P, Z, and Z]{bert_self_attention}
Luo X, Ding H, Tang M, Gandhi P, Zhang Z, and He~Z.
\newblock Attention mechanism with bert for content annotation and
  categorization of pregnancy-related questions on a community q and a site.
\newblock \emph{PubMed Central}, 2020.

\bibitem[Chen et~al.(2021)Chen, Salem, Chen, Backes, Ma, Shen, Wu, and
  Zhang]{BadNL}
Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni
  Shen, Zhonghai Wu, and Yang Zhang.
\newblock {BadNL}: Backdoor attacks against {NLP} models with
  semantic-preserving improvements, dec 2021.
\newblock URL \url{https://doi.org/10.1145%2F3485832.3485837}.

\bibitem[Liu et~al.(2020)Liu, Ma, Bailey, and Lu]{DBLP:2007.02343}
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu.
\newblock Reflection backdoor: {A} natural backdoor attack on deep neural
  networks.
\newblock \emph{CoRR}, abs/2007.02343, 2020.
\newblock URL \url{https://arxiv.org/abs/2007.02343}.

\bibitem[Carlini et~al.(2020)Carlini, Tram{\`{e}}r, Wallace, Jagielski,
  Herbert{-}Voss, Lee, Roberts, Brown, Song, Erlingsson, Oprea, and
  Raffel]{DBLP:2012.07805}
Nicholas Carlini, Florian Tram{\`{e}}r, Eric Wallace, Matthew Jagielski, Ariel
  Herbert{-}Voss, Katherine Lee, Adam Roberts, Tom~B. Brown, Dawn Song,
  {\'{U}}lfar Erlingsson, Alina Oprea, and Colin Raffel.
\newblock Extracting training data from large language models.
\newblock \emph{CoRR}, abs/2012.07805, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.07805}.

\bibitem[Dathathri et~al.(2019)Dathathri, Madotto, Lan, Hung, Frank, Molino,
  Yosinski, and Liu]{PlugNPlay}
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
  Molino, Jason Yosinski, and Rosanne Liu.
\newblock Plug and play language models: {A} simple approach to controlled text
  generation.
\newblock \emph{CoRR}, abs/1912.02164, 2019.
\newblock URL \url{http://arxiv.org/abs/1912.02164}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and Sutskever]{GPT}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training, 2018.

\bibitem[Hossain and Oates(2022)]{CW_Weights}
Khondoker~Murad Hossain and Time Oates.
\newblock Backdoor attack detection in computer vision by applying matrix
  factorization on the weights of deep networks, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.08121}.

\bibitem[Hanu and {Unitary team}(2020)]{Detoxify}
Laura Hanu and {Unitary team}.
\newblock Detoxify.
\newblock Github. https://github.com/unitaryai/detoxify, 2020.

\bibitem[cjadams et~al.(2017)cjadams, Sorensen, Elliott, Dixon, McDonald,
  nithum, and Cukierski]{jigsaw}
cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum,
  and Will Cukierski.
\newblock Toxic comment classification challenge, 2017.
\newblock URL
  \url{https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge}.

\end{thebibliography}
