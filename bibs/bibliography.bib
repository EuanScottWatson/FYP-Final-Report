@misc{BadNL,
  doi       = {10.1145/3485832.3485837},
  url       = {https://doi.org/10.1145%2F3485832.3485837},
  year      = 2021,
  month     = {dec},
  publisher = {{ACM}
               },
  author    = {Xiaoyi Chen and Ahmed Salem and Dingfan Chen and Michael Backes and Shiqing Ma and Qingni Shen and Zhonghai Wu and Yang Zhang},
  title     = {{BadNL}: Backdoor Attacks against {NLP} Models with Semantic-preserving Improvements},
  booktitle = {Annual Computer Security Applications Conference}
}

@article{DBLP:2007.02343,
  author     = {Yunfei Liu and
                Xingjun Ma and
                James Bailey and
                Feng Lu},
  title      = {Reflection Backdoor: {A} Natural Backdoor Attack on Deep Neural Networks},
  journal    = {CoRR},
  volume     = {abs/2007.02343},
  year       = {2020},
  url        = {https://arxiv.org/abs/2007.02343},
  eprinttype = {arXiv},
  eprint     = {2007.02343},
  timestamp  = {Tue, 10 Nov 2020 13:57:47 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2007-02343.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

 @misc{ChatGPT,
  title  = {{ChatGPT}},
  url    = {https://chat.openai.com/},
  author = {OpenAI},
  year   = {2022}
} 

@misc{BERT,
  doi       = {10.48550/ARXIV.1810.04805},
  url       = {https://arxiv.org/abs/1810.04805},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{RoBERTa,
  doi       = {10.48550/ARXIV.1907.11692},
  url       = {https://arxiv.org/abs/1907.11692},
  author    = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ALBERT,
  doi       = {10.48550/ARXIV.1909.11942},
  url       = {https://arxiv.org/abs/1909.11942},
  author    = {Zhenzhong Lan and
               Mingda Chen  and
               Sebastian Goodman and
               Kevin Gimpel and
               Piyush Sharma and
               Radu Soricut},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:2012.07805,
  author     = {Nicholas Carlini and
                Florian Tram{\`{e}}r and
                Eric Wallace and
                Matthew Jagielski and
                Ariel Herbert{-}Voss and
                Katherine Lee and
                Adam Roberts and
                Tom B. Brown and
                Dawn Song and
                {\'{U}}lfar Erlingsson and
                Alina Oprea and
                Colin Raffel},
  title      = {Extracting Training Data from Large Language Models},
  journal    = {CoRR},
  volume     = {abs/2012.07805},
  year       = {2020},
  url        = {https://arxiv.org/abs/2012.07805},
  eprinttype = {arXiv},
  eprint     = {2012.07805},
  timestamp  = {Sat, 02 Jan 2021 15:43:30 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2012-07805.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{CW_Weights,
  doi       = {10.48550/ARXIV.2212.08121},
  url       = {https://arxiv.org/abs/2212.08121},
  author    = {Khondoker Murad Hossain and 
               Time Oates},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{PlugNPlay,
  author     = {Sumanth Dathathri and
                Andrea Madotto and
                Janice Lan and
                Jane Hung and
                Eric Frank and
                Piero Molino and
                Jason Yosinski and
                Rosanne Liu},
  title      = {Plug and Play Language Models: {A} Simple Approach to Controlled Text
                Generation},
  journal    = {CoRR},
  volume     = {abs/1912.02164},
  year       = {2019},
  url        = {http://arxiv.org/abs/1912.02164},
  eprinttype = {arXiv},
  eprint     = {1912.02164},
  timestamp  = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1912-02164.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{GPT,
  added-at  = {2020-07-14T16:37:42.000+0200},
  author    = {Alec Radford and
               Karthik Narasimhan and
               Tim Salimans and
               Ilya Sutskever},
  biburl    = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords  = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title     = {Improving language understanding by generative pre-training},
  year      = 2018
}

@misc{Detoxify,
  title        = {Detoxify},
  author       = {Hanu, Laura and {Unitary team}},
  howpublished = {Github. https://github.com/unitaryai/detoxify},
  year         = {2020}
}

@misc{jigsaw,
  author    = {cjadams and 
               Jeffrey Sorensen and 
               Julia Elliott and 
               Lucas Dixon and 
               Mark McDonald and 
               nithum and 
               Will Cukierski},
  title     = {Toxic Comment Classification Challenge},
  publisher = {Kaggle},
  year      = {2017},
  url       = {https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge}
}

@article{bert_self_attention,
  title     = {Attention Mechanism with BERT for Content Annotation and Categorization of Pregnancy-Related Questions on a Community Q and A Site},
  author    = {Luo X and
               Ding H and
               Tang M and
               Gandhi P and
               Zhang Z and
               He Z},
  journal   = {PubMed Central},
  year      = {2020},
  publisher = {National Library of Medicine}
}

@misc{chat_gpt_environment,
  title        = {The Carbon Footprint of ChatGPT},
  author       = {Kasper Groes Albin Ludvigsen},
  howpublished = {Towards Data Science},
  url          = {https://towardsdatascience.com/the-carbon-footprint-of-chatgpt-66932314627d#:~:text=Using%20the%20ML%20CO2%20Impact,carbon%20footprint%20to%2023.04%20kgCO2e.},
  year         = {2022}
}

@misc{indian-protest-dataset,
  author       = {Pratham Sharma},
  title        = {Farmers Protest Tweets Dataset (CSV)},
  howpublished = {Kaggle},
  url          = {https://www.kaggle.com/datasets/prathamsharma123/farmers-protest-tweets-dataset-csv},
  year         = {2021}
}

@misc{ukraine-war-dataset,
  author       = {Daria Purtova},
  title        = {Russia-Ukraine war - Tweets Dataset (65 days)},
  howpublished = {Kaggle},
  url          = {https://www.kaggle.com/datasets/foklacu/ukraine-war-tweets-dataset-65-days},
  year         = {2022}
}

@misc{OOTB-SA,
  title        = {10 Best Python Libraries for Sentiment Analysis},
  author       = {Alex McFarland},
  howpublished = {Unite.AI},
  url          = {https://www.unite.ai/10-best-python-libraries-for-sentiment-analysis/},
  year         = {2022}
}

@article{Transformer-SA,
  author        = {Francesco Barbieri and 
                   Luis Espinosa Anke and 
                   Jose Camacho-Collados},
  title         = {XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond},
  year          = {2022},
  journal       = {Cardiff NLP},
  eprint        = {2104.12250},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{ABSA,
  author     = {Heng Yang and
                Biqing Zeng and
                Mayi Xu and
                Tianxing Wang},
  title      = {Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable
                Sentiment Dependency Learning},
  journal    = {CoRR},
  volume     = {abs/2110.08604},
  year       = {2021},
  url        = {https://arxiv.org/abs/2110.08604},
  eprinttype = {arXiv},
  eprint     = {2110.08604},
  timestamp  = {Fri, 22 Oct 2021 13:33:09 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2110-08604.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ABSA-paper,
  title     = {Utilizing {BERT} for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence},
  author    = {Sun, Chi  and
               Huang, Luyao  and
               Qiu, Xipeng},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1035},
  doi       = {10.18653/v1/N19-1035},
  pages     = {380--385},
  abstract  = {Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets. The source codes are available at https://github.com/HSLCY/ABSA-BERT-pair.}
}


@article{ZS,
  author     = {Mike Lewis and
                Yinhan Liu and
                Naman Goyal and
                Marjan Ghazvininejad and
                Abdelrahman Mohamed and
                Omer Levy and
                Veselin Stoyanov and
                Luke Zettlemoyer},
  title      = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
                Generation, Translation, and Comprehension},
  journal    = {CoRR},
  volume     = {abs/1910.13461},
  year       = {2019},
  url        = {http://arxiv.org/abs/1910.13461},
  eprinttype = {arXiv},
  eprint     = {1910.13461},
  timestamp  = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{VADER,
  title        = {VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text},
  volume       = {8},
  url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/14550},
  doi          = {10.1609/icwsm.v8i1.14550},
  abstractnote = { &lt;p&gt; The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks. &lt;/p&gt; },
  number       = {1},
  journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
  author       = {Hutto, C. and Gilbert, Eric},
  year         = {2014},
  month        = {May},
  pages        = {216-225}
}

@inproceedings{Radford2019LanguageMA,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Alec Radford and 
            Jeff Wu and
            Rewon Child and
            David Luan and
            Dario Amodei and
            Ilya Sutskever},
  year   = {2019}
}